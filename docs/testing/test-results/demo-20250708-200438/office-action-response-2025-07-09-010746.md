<!--
Generated by CaseThread CLI POC
Document Type: office-action-response
Input File: tfs-02-office-action-alice.yaml
Generated: 2025-07-09T01:07:46.531Z
Generation Time: 39s
-->

IN THE UNITED STATES PATENT AND TRADEMARK OFFICE  

In re Application of: Dr. Lisa Wang et al.  
Application No.: 17/234,567  
Filed: October 20, 2023  
For: System and Method for Predictive Cache Optimization in Distributed Computing Environments Using Machine Learning  
Examiner: Thomas Chen  
Art Unit: 2142  

# RESPONSE TO OFFICE ACTION


## Section 1: Header  
(See above)


## Section 2: Introduction  

Commissioner for Patents  
P.O. Box 1450  
Alexandria, VA 22313-1450  

Dear Sir:

In response to the Office Action mailed January 20, 2024, please consider the following amendments and remarks.


## Section 3: Amendments to the Claims  

### AMENDMENTS TO THE CLAIMS  

Please amend the claims as follows (deletions shown in [brackets]; additions underlined):

**Claim 1 (currently amended):**

1. A computer-implemented method for predictive cache optimization in a distributed computing environment, the method comprising:  
   (a) receiving workload telemetry from a plurality of compute nodes;  
   (b) processing the telemetry with a predictive model to forecast future data requests;  
   (c) pre-populating cache entries based on the forecasted requests;  
   (d) **_executing steps (a)–(c) on hardware that includes at least one graphics processing unit (GPU) configured to perform ≥10^9 matrix-multiplication operations per second and an RDMA-enabled network controller providing sub-10 µs node-to-node latency;_**  
   (e) maintaining cache coherency across the plurality of compute nodes; and  
   (f) evaluating performance metrics to adaptively adjust a window size for the predictive model,  
   wherein the predictive model comprises an **_LSTM neural network having 128-256-128-64 neurons in successive layers_**, and the method provides **_a cache-hit rate of ≥94 % and an average network latency reduction of at least 47 ms._**

(Claims 2–20 depend directly or indirectly from amended Claim 1 and will be interpreted in light of the foregoing amendments.)


## Section 4: Status of Claims  

### STATUS OF THE CLAIMS  

Claims 1-20 were rejected in the Office Action.  
Claims 1 have been amended.  
No claims have been cancelled.  
No new claims have been added.  
Claims 1-20 are currently pending.


## Section 5: Interview Summary  

*(No examiner interview has yet been conducted; however, Applicant is amenable to an interview should the Examiner believe one would expedite prosecution.)*


## Section 6: Response to Rejections  

### RESPONSE TO REJECTIONS  

I. Response to 35 U.S.C. § 101 Rejection – Alleged Abstract Idea  

Claims 1-20 were rejected under 35 U.S.C. § 101 as being directed to an abstract idea involving mental processes and mathematical concepts (Alice step 2A, prongs 1 and 2) and lacking an “inventive concept” (Alice step 2B).

Applicant respectfully traverses this rejection for the reasons set forth below and in the accompanying Remarks.  The amended claims recite specific, non-generic computer hardware—including GPUs capable of ≥10^9 operations per second and RDMA-enabled network controllers providing sub-10 µs latency—that integrate with a novel LSTM-based predictive model and a modified Raft cache-coherency protocol to yield a technological improvement in distributed computing systems.  As such, the claims are directed to a practical application that is not merely abstract and, at a minimum, include significantly more than any alleged abstract idea.


## Section 7: Detailed Arguments  

### REMARKS  

1. **Alice Step 2A, Prong 1 – Claim Construction**  
   The claims, as amended, are directed to a “computer-implemented method” executed on expressly recited hardware (GPUs and RDMA controllers).  The predictive model is not a mere mathematical concept; it is embodied in a specific LSTM architecture (128-256-128-64 neurons) implemented in silicon and configured to process real-time telemetry at industrial scale.

2. **Alice Step 2A, Prong 2 – Integration into a Practical Application**  
   a. The amended claims require hardware acceleration that cannot reasonably be performed in the human mind.  
   b. The GPU and RDMA limitations ground the predictive operations firmly in the physical realm, providing a concrete technological solution to the long-felt problem of network latency exceeding 100 ms.  
   c. Empirical benchmark data (see enclosed Exhibit B) demonstrate a 94 % cache-hit rate and a 47 ms latency reduction—results unattainable by prior art software-only approaches.

3. **Alice Step 2B – Inventive Concept**  
   a. **Novel LSTM Architecture**: The specific four-layer configuration yields superior temporal-spatial prediction accuracy, as supported by the Declaration of Dr. Lisa Wang.  
   b. **Modified Raft Protocol**: Integrates dynamic window sizing with RDMA to maintain coherency without sacrificing throughput.  
   c. **Synergistic Hardware Integration**: The claimed configuration exploits the parallelism of modern GPUs and the low latency of RDMA, producing unexpected performance gains—over 40 % better than theoretical predictions.

4. **Secondary Considerations Underscoring Non-Obviousness and Patent-Eligibility**  
   • **Commercial Success**: CloudGiant’s $500 K license and Applicant’s oversubscribed Series B investment round.  
   • **Long-Felt Need & Failure of Others**: Industry leaders have unsuccessfully attempted to resolve distributed cache latency for over a decade.  
   • **Unexpected Results**: Independent benchmarks confirm improvements beyond the state-of-the-art and prior academic models (see SIGCOMM 2023 Best Paper Award).

5. **Case Law and MPEP Guidance**  
   • *Enfish* (Fed. Cir. 2016) – Claims directed to an improvement in computer functioning are not abstract.  
   • *DDR Holdings* (Fed. Cir. 2014) – Claims rooted in computer technology that overcome a problem specifically arising in the realm of computer networks are patent-eligible.  
   In line with these precedents and MPEP § 2106.05(a)–(c), the present claims recite a specific technique that improves the functioning of distributed computing systems and therefore fall squarely within patent-eligible subject matter.

6. **Clarifying Support in Specification**  
   The Specification (¶¶ 45-63, Figs. 3-5) expressly supports each newly added limitation, including GPU specifications, RDMA controller parameters, and LSTM layer sizes.  Accordingly, no new matter has been introduced.

For at least the reasons above, the § 101 rejection is respectfully withdrawn.


## Section 8: Conclusion  

### CONCLUSION  

In view of the foregoing amendments and remarks, Applicant respectfully submits that all claims are in condition for allowance.  Applicant believes that no outstanding issues remain and respectfully requests that the Examiner indicate allowance of the application at the Examiner’s earliest convenience.

If the Examiner believes that a telephone interview would expedite prosecution of this application, the Examiner is invited to contact the undersigned at the telephone number provided below.


## Section 9: Signature  

Respectfully submitted,

/Sarah Chen/  
Sarah Chen  
Registration No. 72,345  
Innovatech Law Group PLLC  
456 Market Street, Suite 900  
San Francisco, CA 94105  
(415) 555-1234  
schen@innovatechlaw.com  

Date: June 09, 2024